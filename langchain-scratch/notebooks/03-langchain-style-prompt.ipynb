{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc46e50637af7f6",
   "metadata": {},
   "source": [
    "# 3. 랭체인 스타일로 프롬프트 작성하는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7161adeea8302761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:33:23.127569Z",
     "start_time": "2025-11-10T22:33:23.052417Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model =\"deepseek-r1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebbc7c9c9541e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:37:43.283364Z",
     "start_time": "2025-11-10T22:37:43.255560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"{country}의 수도는 어디야?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({'country': '오스트레일리아'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b7633204461d78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:37:45.293157Z",
     "start_time": "2025-11-10T22:37:45.289985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='오스트레일리아의 수도는 어디야?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10d32ff9ba2f6e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:38:06.172278Z",
     "start_time": "2025-11-10T22:37:57.455668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='오스트레일리아의 수도는 **캔버라(Canberra)**입니다.  \\n**캔버라**는 **1912년**에 **시드니**와 **멜버른**의 권力争합으로 인해 새로 지정된 수도로, 1920년부터 공식적으로 사용되고 있습니다.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-11-10T22:55:06.497172Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7390986583, 'load_duration': 54443291, 'prompt_eval_count': 14, 'prompt_eval_duration': 821344458, 'eval_count': 277, 'eval_duration': 6466117961, 'model_name': 'deepseek-r1:8b', 'model_provider': 'ollama'}, id='lc_run--d3ffc270-82af-4a4e-832c-173ae753625d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 277, 'total_tokens': 291})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7869b2895856214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-11-10T22:55:15.61389Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9105140041, 'load_duration': 44509458, 'prompt_eval_count': 39, 'prompt_eval_duration': 182642584, 'eval_count': 379, 'eval_duration': 8817993953, 'model_name': 'deepseek-r1:8b', 'model_provider': 'ollama'}, id='lc_run--7240e7ad-6062-4877-a248-f75648a779ad-0', usage_metadata={'input_tokens': 39, 'output_tokens': 379, 'total_tokens': 418})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that can answer questions about the world.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579ab05",
   "metadata": {},
   "source": [
    "- 아래의 `ChatPromptTemplate` 방식이 조금더 랭체인스러운 방식이라고 하면서 권장함\n",
    "- message type 에는 system, human, ai, tool 등이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8123f327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that can answer questions about the world.', additional_kwargs={}, response_metadata={}), HumanMessage(content='오스트레일리아의 수도는 어디야?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions about the world.\"),\n",
    "    (\"human\", \"{country}의 수도는 어디야?\"),\n",
    "])\n",
    "\n",
    "chat_prompt = prompt_template.invoke({'country': '오스트레일리아'})\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0cb5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that can answer questions about the world.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='오스트레일리아의 수도는 어디야?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7b93a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='오스트레일리아의 수도는 **캔버라(Canberra)**입니다.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-11-10T22:55:20.617032Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4972262708, 'load_duration': 67889000, 'prompt_eval_count': 27, 'prompt_eval_duration': 102260959, 'eval_count': 206, 'eval_duration': 4767391913, 'model_name': 'deepseek-r1:8b', 'model_provider': 'ollama'}, id='lc_run--a2dfbbc8-5227-493b-b97f-4ae8b61226fa-0', usage_metadata={'input_tokens': 27, 'output_tokens': 206, 'total_tokens': 233})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(chat_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
