"""Music Processing êµìž¬ RAG ì±—ë´‡ (Streamlit + LangGraph)

ì‹¤í–‰:
    uv run streamlit run app/chat.py
"""

from dataclasses import dataclass, field

import ollama
import streamlit as st
from langchain_core.messages import HumanMessage
from langchain_ollama import ChatOllama
from langgraph.graph import END, START, StateGraph
from pymilvus import MilvusClient

EMBEDDING_MODEL = "qwen3-embedding:8b"
LLM_MODEL = "qwen3:14b"
MILVUS_URI = "http://localhost:19530"
DEFAULT_COLLECTION = "music_processing_book"


# --- LangGraph State & Nodes ---


@dataclass
class RAGState:
    query: str = ""
    collection: str = DEFAULT_COLLECTION
    top_k: int = 3
    chapter_filter: str = ""
    documents: list[dict] = field(default_factory=list)
    answer: str = ""


def retrieve(state: RAGState) -> dict:
    client = MilvusClient(uri=MILVUS_URI)
    client.load_collection(state.collection)

    response = ollama.embed(model=EMBEDDING_MODEL, input=[state.query])
    query_vector = response.embeddings[0]

    filter_expr = f'chapter == "{state.chapter_filter}"' if state.chapter_filter else None

    results = client.search(
        collection_name=state.collection,
        data=[query_vector],
        limit=state.top_k,
        output_fields=["text", "heading", "chapter", "chunk_index"],
        search_params={"metric_type": "COSINE"},
        filter=filter_expr,
    )

    documents = []
    for hit in results[0]:
        doc = hit["entity"]
        doc["score"] = hit["distance"]
        documents.append(doc)

    return {"documents": documents}


def generate(state: RAGState) -> dict:
    if not state.documents:
        return {"answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    context_parts = []
    for i, doc in enumerate(state.documents, 1):
        context_parts.append(
            f"[ë¬¸ì„œ {i}] (ì±•í„°: {doc['chapter']}, ì„¹ì…˜: {doc['heading']})\n{doc['text']}"
        )
    context = "\n\n---\n\n".join(context_parts)

    # ëŒ€í™” ížˆìŠ¤í† ë¦¬ êµ¬ì„±
    messages = []
    for msg in st.session_state.get("messages", [])[:-1]:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        else:
            messages.append(HumanMessage(content=msg["content"]))

    messages.append(
        HumanMessage(content=f"""ì•„ëž˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ë˜, ì „ë¬¸ ìš©ì–´ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

## ì°¸ê³  ë¬¸ì„œ
{context}

## ì§ˆë¬¸
{state.query}
""")
    )

    llm = ChatOllama(model=LLM_MODEL)
    response = llm.invoke(messages)
    return {"answer": response.content}


@st.cache_resource
def build_graph():
    graph = StateGraph(RAGState)
    graph.add_node("retrieve", retrieve)
    graph.add_node("generate", generate)
    graph.add_edge(START, "retrieve")
    graph.add_edge("retrieve", "generate")
    graph.add_edge("generate", END)
    return graph.compile()


# --- Streamlit UI ---


st.set_page_config(page_title="Music Processing RAG", page_icon="ðŸŽµ", layout="wide")
st.title("Music Processing êµìž¬ RAG ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    collection = st.text_input("ì»¬ë ‰ì…˜", value=DEFAULT_COLLECTION)
    top_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜", min_value=1, max_value=10, value=3)

    chapters = [
        "", "Chapter 1", "Chapter 2", "Chapter 3", "Chapter 4",
        "Chapter 5", "Chapter 6", "Chapter 7", "Chapter 8",
    ]
    chapter_filter = st.selectbox("ì±•í„° í•„í„°", chapters, format_func=lambda x: "ì „ì²´" if x == "" else x)

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# ë©”ì‹œì§€ ížˆìŠ¤í† ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "documents" in msg:
            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                for i, doc in enumerate(msg["documents"], 1):
                    st.markdown(f"**{i}. [{doc['chapter']}] {doc['heading']}** (ìœ ì‚¬ë„: {doc['score']:.3f})")
                    st.text(doc["text"][:300] + "..." if len(doc["text"]) > 300 else doc["text"])
                    st.divider()

# ì±„íŒ… ìž…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
            app = build_graph()
            state = RAGState(
                query=prompt,
                collection=collection,
                top_k=top_k,
                chapter_filter=chapter_filter,
            )
            result = app.invoke(state)

        st.markdown(result["answer"])

        if result["documents"]:
            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                for i, doc in enumerate(result["documents"], 1):
                    st.markdown(f"**{i}. [{doc['chapter']}] {doc['heading']}** (ìœ ì‚¬ë„: {doc['score']:.3f})")
                    st.text(doc["text"][:300] + "..." if len(doc["text"]) > 300 else doc["text"])
                    st.divider()

    st.session_state.messages.append({
        "role": "assistant",
        "content": result["answer"],
        "documents": result["documents"],
    })
