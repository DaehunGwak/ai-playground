import os
import numpy as np
import dotenv
import streamlit as st
import plotly.graph_objects as go

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
dotenv.load_dotenv(".env.local", override=True)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¯ Audio Embedding",
    page_icon="ğŸ¯",
    layout="wide",
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700&display=swap');
    
    .stApp {
        font-family: 'Outfit', sans-serif;
    }
    
    .main-header {
        background: linear-gradient(135deg, #0f0c29 0%, #302b63 50%, #24243e 100%);
        padding: 2rem;
        border-radius: 16px;
        margin-bottom: 2rem;
        text-align: center;
    }
    
    .main-header h1 {
        color: #00d9ff;
        font-size: 2.5rem;
        font-weight: 700;
        margin: 0;
        text-shadow: 0 0 30px rgba(0, 217, 255, 0.5);
    }
    
    .main-header p {
        color: #b8c5d6;
        font-size: 1.1rem;
        margin-top: 0.5rem;
    }
    
    .info-card {
        background: linear-gradient(145deg, #1e1e2e, #2d2d44);
        border: 1px solid #3d3d5c;
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1rem;
    }
    
    .metric-value {
        color: #00d9ff;
        font-size: 1.8rem;
        font-weight: 700;
    }
    
    .metric-label {
        color: #b8c5d6;
        font-size: 0.9rem;
    }
    
    .embedding-stats {
        background: linear-gradient(145deg, #1a1a2e, #252545);
        border-radius: 12px;
        padding: 1rem;
        margin: 1rem 0;
    }
    
    .stat-row {
        display: flex;
        justify-content: space-between;
        padding: 0.5rem 0;
        border-bottom: 1px solid #3d3d5c;
    }
    
    .stat-row:last-child {
        border-bottom: none;
    }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ¯ Audio Embedding Extractor</h1>
    <p>Extract CLAP embeddings from audio files using laion/larger_clap_music</p>
</div>
""", unsafe_allow_html=True)


# CLAP ëª¨ë¸ ë¡œë“œ (ìºì‹±)
@st.cache_resource
def load_clap_model():
    """CLAP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    from transformers import ClapProcessor, ClapModel
    
    processor = ClapProcessor.from_pretrained("laion/larger_clap_music")
    model = ClapModel.from_pretrained("laion/larger_clap_music")
    return processor, model


def extract_embedding(audio_path: str, processor, model):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ CLAP ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    import librosa
    import torch
    
    TARGET_SR = 48000
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ (48kHz, mono)
    audio_data, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)
    
    # processorë¡œ ì „ì²˜ë¦¬
    inputs = processor(
        audio=audio_data,
        sampling_rate=TARGET_SR,
        return_tensors="pt"
    )
    
    # ì„ë² ë”© ì¶”ì¶œ
    with torch.no_grad():
        audio_emb = model.get_audio_features(**inputs)
    
    return audio_emb.cpu().numpy(), audio_data, sr


def plot_embedding_line(embedding: np.ndarray):
    """ì„ë² ë”© ë²¡í„°ë¥¼ ì„  ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=list(range(len(embedding.flatten()))),
        y=embedding.flatten(),
        mode='lines',
        line=dict(color='#00d9ff', width=1),
        fill='tozeroy',
        fillcolor='rgba(0, 217, 255, 0.2)'
    ))
    fig.update_layout(
        title="Embedding Vector (512 dimensions)",
        xaxis_title="Dimension",
        yaxis_title="Value",
        plot_bgcolor='rgba(30,30,46,0.8)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color='#b8c5d6'),
        title_font=dict(color='#00d9ff', size=16),
        xaxis=dict(gridcolor='#3d3d5c'),
        yaxis=dict(gridcolor='#3d3d5c')
    )
    return fig


# ì‚¬ì´ë“œë°” - íŒŒì¼ ì„ íƒ
with st.sidebar:
    st.header("ğŸµ Select Audio File")
    
    # íŒŒì¼ ì†ŒìŠ¤ ì„ íƒ
    source_type = st.radio(
        "Choose source",
        ["Local Files (suno_mono)", "Upload File"]
    )
    
    selected_file = None
    uploaded_file = None
    
    if source_type == "Local Files (suno_mono)":
        # ë¡œì»¬ suno_mono ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡
        mono_dir = "static_files/suno_mono"
        if os.path.exists(mono_dir):
            mp3_files = sorted([f for f in os.listdir(mono_dir) if f.lower().endswith('.mp3')])
            if mp3_files:
                selected_file = st.selectbox(
                    "Select a track",
                    options=mp3_files,
                    format_func=lambda x: x.replace('.mp3', '')
                )
            else:
                st.warning("No MP3 files found in suno_mono directory")
        else:
            st.warning("suno_mono directory not found")
    else:
        uploaded_file = st.file_uploader(
            "Upload an MP3 file",
            type=['mp3', 'wav', 'flac', 'ogg'],
            help="Supported formats: MP3, WAV, FLAC, OGG"
        )

# ë©”ì¸ ì»¨í…ì¸ 
if selected_file or uploaded_file:
    # ëª¨ë¸ ë¡œë“œ ìƒíƒœ í‘œì‹œ
    with st.spinner("ğŸ”„ Loading CLAP model... (this may take a while on first run)"):
        try:
            processor, model = load_clap_model()
            model_loaded = True
        except Exception as e:
            st.error(f"âŒ Failed to load model: {e}")
            model_loaded = False
    
    if model_loaded:
        # íŒŒì¼ ê²½ë¡œ ê²°ì •
        if selected_file:
            audio_path = os.path.join("static_files/suno_mono", selected_file)
            file_name = selected_file
        else:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
                tmp.write(uploaded_file.read())
                audio_path = tmp.name
            file_name = uploaded_file.name
        
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        st.markdown(f"### ğŸµ Selected: **{file_name.replace('.mp3', '')}**")
        
        # ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´
        if selected_file:
            st.audio(audio_path)
        else:
            uploaded_file.seek(0)
            st.audio(uploaded_file)
        
        # ì„ë² ë”© ì¶”ì¶œ ë²„íŠ¼
        if st.button("ğŸš€ Extract Embedding", type="primary", use_container_width=True):
            with st.spinner("ğŸ”„ Extracting embedding..."):
                try:
                    embedding, audio_data, sr = extract_embedding(audio_path, processor, model)
                    
                    # ì„ë² ë”© í†µê³„ ê³„ì‚°
                    emb_flat = embedding.squeeze()
                    l2_norm = np.linalg.norm(emb_flat)
                    
                    st.success("âœ… Embedding extracted successfully!")
                    
                    # ì˜¤ë””ì˜¤ ì •ë³´
                    st.markdown("---")
                    st.markdown("### ğŸ“Š Audio Information")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Sample Rate", f"{sr} Hz")
                    with col2:
                        st.metric("Duration", f"{len(audio_data) / sr:.2f}s")
                    with col3:
                        st.metric("Total Samples", f"{len(audio_data):,}")
                    with col4:
                        st.metric("Channels", "Mono")
                    
                    # ì„ë² ë”© í†µê³„
                    st.markdown("---")
                    st.markdown("### ğŸ¯ Embedding Statistics")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Dimensions", embedding.shape[-1])
                    with col2:
                        st.metric("L2 Norm", f"{l2_norm:.6f}")
                    with col3:
                        st.metric("Min Value", f"{emb_flat.min():.4f}")
                    with col4:
                        st.metric("Max Value", f"{emb_flat.max():.4f}")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Mean", f"{emb_flat.mean():.6f}")
                    with col2:
                        st.metric("Std Dev", f"{emb_flat.std():.6f}")
                    with col3:
                        st.metric("Median", f"{np.median(emb_flat):.6f}")
                    with col4:
                        st.metric("Non-zero", f"{np.count_nonzero(emb_flat)}")
                    
                    # ì‹œê°í™”
                    st.markdown("---")
                    st.markdown("### ğŸ“ˆ Visualization")
                    st.plotly_chart(plot_embedding_line(embedding), use_container_width=True)
                    
                    # Raw ì„ë² ë”© ë°ì´í„° í‘œì‹œ
                    st.markdown("---")
                    st.markdown("### ğŸ”¢ Raw Embedding Data")
                    with st.expander("View raw embedding vector (512 values)", expanded=False):
                        st.code(f"Shape: {embedding.shape}\n\nValues:\n{emb_flat.tolist()}")
                    
                    # ë‹¤ìš´ë¡œë“œ ì˜µì…˜
                    st.markdown("---")
                    st.markdown("### ğŸ’¾ Export")
                    col1, col2 = st.columns(2)
                    with col1:
                        # NumPy í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
                        import io
                        buffer = io.BytesIO()
                        np.save(buffer, emb_flat)
                        buffer.seek(0)
                        st.download_button(
                            label="ğŸ“¥ Download as .npy",
                            data=buffer,
                            file_name=f"{file_name.replace('.mp3', '')}_embedding.npy",
                            mime="application/octet-stream"
                        )
                    with col2:
                        # JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
                        import json
                        json_data = json.dumps({
                            "file_name": file_name,
                            "embedding": emb_flat.tolist(),
                            "l2_norm": float(l2_norm),
                            "dimensions": int(embedding.shape[-1])
                        }, indent=2)
                        st.download_button(
                            label="ğŸ“¥ Download as .json",
                            data=json_data,
                            file_name=f"{file_name.replace('.mp3', '')}_embedding.json",
                            mime="application/json"
                        )
                    
                except Exception as e:
                    st.error(f"âŒ Error extracting embedding: {e}")
                    import traceback
                    st.code(traceback.format_exc())
        
        # ì—…ë¡œë“œëœ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if uploaded_file and 'audio_path' in locals():
            try:
                os.unlink(audio_path)
            except:
                pass

else:
    # ì•ˆë‚´ ë©”ì‹œì§€
    st.info("ğŸ‘ˆ Select an audio file from the sidebar to extract its embedding")
    
    # CLAP ëª¨ë¸ ì •ë³´
    st.markdown("---")
    st.markdown("### â„¹ï¸ About CLAP Embeddings")
    st.markdown("""
    <div class="info-card">
        <h4 style="color: #00d9ff;">What is CLAP?</h4>
        <p style="color: #b8c5d6;">
            <strong>CLAP (Contrastive Language-Audio Pretraining)</strong> is a neural network trained to 
            understand the relationship between audio and text. It creates meaningful vector representations 
            (embeddings) of audio that capture semantic information about the sound.
        </p>
        <br>
        <h4 style="color: #00d9ff;">Model: laion/larger_clap_music</h4>
        <p style="color: #b8c5d6;">
            This model is specifically fine-tuned for music understanding. It produces 512-dimensional 
            embeddings that encode musical characteristics like genre, mood, instruments, and more.
        </p>
        <br>
        <h4 style="color: #00d9ff;">Use Cases</h4>
        <ul style="color: #b8c5d6;">
            <li>Music similarity search</li>
            <li>Music recommendation systems</li>
            <li>Audio classification</li>
            <li>Cross-modal retrieval (text-to-audio, audio-to-text)</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #666;'>Powered by ğŸ¤— Transformers & CLAP</div>",
    unsafe_allow_html=True
)
